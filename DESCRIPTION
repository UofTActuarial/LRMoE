Package: LRMoECPNew
Type: Package
Title: Logit-Weighted Reduced Mixture-of-Experts
Version: 0.1.0
Author: Spark Tseung [aut, cre],
  Tsz Chai Fung [aut],
  Andrei L. Badescu [aut],
  Sheldon X. Lin [aut]
Maintainer: Spark Tseung <spark.tseung@mail.utoronto.ca>
Description: The Logit-Weighted Reduced Mixture-of-Experts (LRMoE) 
  proposed by Fung et al. (2019) is a flexible framework actuarial loss modelling.
  For more details, see Fung et al. (2019) "A class of mixture of experts models for general insurance: Theoretical developments"
  published in Insurance: Mathematics and Economics, and
  Fung et al. (2019) "A CLASS OF MIXTURE OF EXPERTS MODELS FOR GENERAL INSURANCE: APPLICATION TO CORRELATED CLAIM FREQUENCIES"
  in ASTIN Bulletin: The Journal of the IAA.
License: GPL (>= 2)
Encoding: UTF-8
LazyData: true
Depends: R (>= 3.5)
Imports: 
    Rcpp,
    actuar,
    copula,
    expint,
    EnvStats,
    ggplot2,
    matrixStats,
    NMOF,
    reshape2,
    rmutil,
    statmod,
    stats
RoxygenNote: 7.1.1
LinkingTo: 
    Rcpp
